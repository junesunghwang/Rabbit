# pip install streamlit rank_bm25 faiss-cpu sentence-transformers langchain-huggingface PyPDF2
# pip install langchain langchain-community langchain-openai python-dotenv

import os
import streamlit as st
from PyPDF2 import PdfReader
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.retrievers import EnsembleRetriever
from langchain_openai import ChatOpenAI


# 0) í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
PDF_PATH = "í† ë¼ì™€_ê±°ë¶ì´_ì¤„ê±°ë¦¬.pdf"

# 1) PDFì—ì„œ ë¬¸ì„œ ë¡œë“œ
def load_documents_from_pdf(pdf_path: str):
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"{pdf_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    reader = PdfReader(pdf_path)
    texts, metadatas = [], []

    for page_num, page in enumerate(reader.pages, start=1):
        content = page.extract_text()
        if not content or not content.strip():
            continue
        texts.append(content.strip())
        metadatas.append({
            "page": page_num,
            "source": f"pdf_page_{page_num}",
            "title": "í† ë¼ì™€ ê±°ë¶ì´"
        })

    return texts, metadatas

# 2) ì•™ìƒë¸” Retriever êµ¬ì„± (BM25 + FAISS)
def build_ensemble_retriever(texts, metadatas):
    bm25 = BM25Retriever.from_texts(texts, metadatas=metadatas)
    bm25.k = 2

    embedding = HuggingFaceEmbeddings(model_name="distiluse-base-multilingual-cased-v1")
    faiss_store = FAISS.from_texts(texts, embedding, metadatas=metadatas)
    faiss = faiss_store.as_retriever(search_kwargs={"k": 2})

    return EnsembleRetriever(retrievers=[bm25, faiss], weights=[0.3, 0.7])

# 3) OpenAI LLM ë¡œë“œ
@st.cache_resource(show_spinner=False)
def load_openai_llm(model_name="gpt-4o-mini", temperature=0.0):
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”")
    return ChatOpenAI(model=model_name, temperature=temperature, api_key=api_key)

# 4) ê²€ìƒ‰ í•¨ìˆ˜
def search(query, retriever):
    return retriever.invoke(query) or []

# 5) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
def build_prompt(query, docs):
    lines = []
    lines.append("ì•„ë˜ 'ìë£Œ'ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”.")
    lines.append("- ìë£Œ ë°– ì •ë³´ë¥¼ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.")
    lines.append("- ë‹µí•  ìˆ˜ ì—†ìœ¼ë©´ 'ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'ë¼ê³  ë§í•˜ì„¸ìš”.\n")
    lines.append(f"ì§ˆë¬¸:\n{query}\n")
    lines.append("ìë£Œ:")
    for i, d in enumerate(docs, 1):
        m = d.metadata
        lines.append(f"[ë¬¸ì„œ{i}] (source={m.get('source')}, page={m.get('page')}, title={m.get('title')})\n{d.page_content}\n")
    lines.append("ë‹µë³€:")
    return "\n".join(lines)

# 6) LLM í˜¸ì¶œ
def generate_with_llm(llm, prompt):
    resp = llm.invoke(prompt)
    return resp.content.strip()

# 7) Streamlit ì•±
def main():
    st.set_page_config(page_title="ğŸ“„ PDF RAG ì–´ì‹œìŠ¤í„´íŠ¸", page_icon="ğŸ“„", layout="centered")
    st.title("ğŸ“„ í† ë¼ì™€ ê±°ë¶ì´ ì¤„ê±°ë¦¬ Q&A")

    try:
        texts, metadatas = load_documents_from_pdf(PDF_PATH)
    except FileNotFoundError as e:
        st.error(str(e))
        st.stop()

    if "retriever" not in st.session_state:
        st.session_state.retriever = build_ensemble_retriever(texts, metadatas)
    if "llm" not in st.session_state:
        st.session_state.llm = load_openai_llm()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_input = st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”.")
    if user_input:
        st.chat_message("user").markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})

        docs = search(user_input.strip(), st.session_state.retriever)

        if not docs:
            answer = "ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        else:
            prompt = build_prompt(user_input.strip(), docs)
            answer = generate_with_llm(st.session_state.llm, prompt)

        with st.chat_message("assistant"):
            st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})

        with st.expander("ğŸ” ì‚¬ìš©í•œ ìë£Œ ë³´ê¸°"):
            if not docs:
                st.markdown("_ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ_")
            else:
                for i, d in enumerate(docs, 1):
                    m = d.metadata
                    st.markdown(f"**[ë¬¸ì„œ{i}]** (source={m.get('source')}, page={m.get('page')}, title={m.get('title')})\n\n{d.page_content}")

if __name__ == "__main__":
    main()
